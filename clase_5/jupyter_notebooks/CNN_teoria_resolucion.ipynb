{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1WE6TyEWxs4wUoegDtbyRWzcv7Y42A3h7","timestamp":1685544515036}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_wIQ8hjDpdVi"},"source":["## Importar lo necesario"]},{"cell_type":"code","metadata":{"id":"uHQUjDs12DLW","executionInfo":{"status":"ok","timestamp":1717028589930,"user_tz":180,"elapsed":7773,"user":{"displayName":"Marcos Uriel Maillot","userId":"16876029369473785241"}}},"source":["import torch\n","import torchvision\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","from matplotlib import image\n","from matplotlib import pyplot\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Ejercicio - cálculo de dimensiones\n","\n","Implemente con las clases de `conv2d` y `MaxPool2d` los siguientes casos:"],"metadata":{"id":"t7gNlGwwJ9Hs"}},{"cell_type":"markdown","source":["#### ejercicio a)\n","Implemente una capa de convolución de 1 canal de entrada y salida, con kernel de tamaño 5 y un padding que conserve el tamaño de la imagen; seguido de una capa de pooling que reduzca en la mitad las dimensiones de la imagen."],"metadata":{"id":"b0_p3pSsLq-V"}},{"cell_type":"code","source":["conv = torch.nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 7, padding ='same')\n","pool = torch.nn.MaxPool2d(kernel_size = 2, stride=2, padding=0)\n","\n","imagen_ejemplo= torch.rand(1, 1, 100, 100)\n","# dimensiones: (N, #filtros, H_out, W_out)\n","print('tamaño imagen ejemplo')\n","print(imagen_ejemplo.shape)\n","\n","# nuestro \"forward\"\n","x = conv(imagen_ejemplo)\n","print('tamaño salida conv')\n","print(x.shape)\n","\n","salida = pool(x)\n","\n","print('tamaño imagen salida')\n","print(salida.shape)"],"metadata":{"id":"MgHT-wniKq9i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717028593889,"user_tz":180,"elapsed":276,"user":{"displayName":"Marcos Uriel Maillot","userId":"16876029369473785241"}},"outputId":"299becb7-941a-47b4-9cce-1cef61079484"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tamaño imagen ejemplo\n","torch.Size([1, 1, 100, 100])\n","tamaño salida conv\n","torch.Size([1, 1, 100, 100])\n","tamaño imagen salida\n","torch.Size([1, 1, 50, 50])\n"]}]},{"cell_type":"markdown","source":["#### ejercicio b)\n","Implemente una capa de convolución de 10 canales con kernel de tamaño 5, un padding de 0 y stride para que el kernel no se pise a si mismo.\n","\n","¿cual es le tamaño final de la salida, si entra una imagen de 200x100x3?"],"metadata":{"id":"FOrKtKtgMLMW"}},{"cell_type":"code","source":["conv = torch.nn.Conv2d(in_channels = 3, out_channels = 10, kernel_size = 5, padding = 0, stride=5)\n","\n","imagen_ejemplo= torch.rand(1, 3, 200, 100)\n","# dimensiones: (N, #filtros, H_out, W_out)\n","print('tamaño imagen ejemplo')\n","print(imagen_ejemplo.shape)\n","\n","# nuestro \"forward\"\n","salida = conv(imagen_ejemplo)\n","\n","print('tamaño imagen salida')\n","print(salida.shape)"],"metadata":{"id":"mD8qWsHIMx5L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711584880209,"user_tz":180,"elapsed":312,"user":{"displayName":"Marcos Uriel Maillot","userId":"16876029369473785241"}},"outputId":"65baa681-5e05-41d6-c20b-7ab2d7ab07be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tamaño imagen ejemplo\n","torch.Size([1, 3, 200, 100])\n","tamaño imagen salida\n","torch.Size([1, 10, 40, 20])\n"]}]},{"cell_type":"markdown","source":["#### ejercicio c)\n","Implemente 3 capas de convolución en cascada, cada una con kernel size de 7, numero de canales de salida de [8, 16, 32] respectivamente y, con padding de 0. Al final, coloque una capa de pooling con kernel del 7, con un stride para que no se pisen a si mismo.\n","\n","¿cual es le tamaño final de la salida, si entra una imagen de 500x500x1?\n","\n","¿cual es el número de parámetros entenables?"],"metadata":{"id":"txe9or7DMyWO"}},{"cell_type":"code","source":["conv1 = torch.nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = 7, padding = 0) #pierde 3+3 , 3+3\n","conv2 = torch.nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 7, padding = 0) #pierde 3+3 , 3+3\n","conv3 = torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 7, padding = 0) #pierde 3+3 , 3+3\n","pool = torch.nn.MaxPool2d(kernel_size = 7, stride=7, padding=0, ceil_mode=False) # reduce en 7 veces, ver ceil mode!\n","\n","imagen_ejemplo= torch.rand(1, 1, 500, 500)\n","# dimensiones: (N, #filtros, H_out, W_out)\n","print('tamaño imagen ejemplo')\n","print(imagen_ejemplo.shape)\n","\n","# nuestro \"forward\"\n","x = conv1(imagen_ejemplo)\n","x = conv2(x)\n","x = conv3(x)\n","print('tamaño parcial ANTES del pooling')\n","print(x.shape)\n","salida = pool(x)\n","\n","print('tamaño imagen salida')\n","print(salida.shape)"],"metadata":{"id":"K54iYnrLNqxM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717028744864,"user_tz":180,"elapsed":865,"user":{"displayName":"Marcos Uriel Maillot","userId":"16876029369473785241"}},"outputId":"26cfdcb4-8096-46b6-eeaa-34303685ba53"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tamaño imagen ejemplo\n","torch.Size([1, 1, 500, 500])\n","tamaño parcial ANTES del pooling\n","torch.Size([1, 32, 482, 482])\n","tamaño imagen salida\n","torch.Size([1, 32, 68, 68])\n"]}]},{"cell_type":"markdown","source":["#### ejercico d)\n","Si tiene una imagen a procesar de 300x200x3 y desea aplicar 3 capas de convolución+pooling en cascada con las siguientes características:\n","\n","* kernel convolución = 5\n","* padding convolución = 0\n","* stride convolución = 1\n","* canales conv1 = 16\n","* canales conv2 = 32\n","* canales conv3 = 64\n","\n","* kernel pooling = 2\n","* padding pooling = 0\n","* stride pooling = 2\n","\n","¿cual es el tamaño final de la salida y cuantos canales tiene?\n","\n","Si tiene que conectar esa salida a una `fully_connected` para clasificar 10 clases ¿que tamaño debe tener?\n","\n","¿Cúal es el número total de parámetros a entrenar (contando la `fc`)?"],"metadata":{"id":"RsE0aOuFNrpT"}},{"cell_type":"code","source":["conv1 = torch.nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 5, padding = 0) #pierde 2+2 , 2+2\n","conv2 = torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 5, padding = 0) #pierde 2+2 , 2+2\n","conv3 = torch.nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5, padding = 0) #pierde 2+2 ,2+2\n","\n","pool = torch.nn.MaxPool2d(kernel_size = 2, stride=2, padding=0, ceil_mode=False) # reduce en 2 veces, ver ceil mode!\n","\n","imagen_ejemplo= torch.rand(1, 3, 300,200)\n","# dimensiones: (N, #filtros, H_out, W_out)\n","print('tamaño imagen ejemplo')\n","print(imagen_ejemplo.shape)\n","\n","# nuestro \"forward\"\n","x = conv1(imagen_ejemplo)\n","x = pool(x)\n","x = conv2(x)\n","x = pool(x)\n","x = conv3(x)\n","x = pool(x)\n","print('tamaño parcial ANTES de fc')\n","forma = x.shape\n","print(forma)\n","\n","# agrego la fc\n","\n","fc = torch.nn.Linear(in_features = forma[1]*forma[2]*forma[3], out_features = 10)\n","salida = fc(x.view(x.shape[0], -1))\n","print()\n","print('salida final de la fc')\n","print(salida.shape)\n","\n","# numero de parametros\n","total = torch.tensor([0])\n","print('--'*20)\n","print('--'*20)\n","for layer in [conv1, conv2, conv3, fc]:\n","  kernel = torch.tensor(layer.weight.shape)\n","  bias = torch.tensor(layer.bias.shape)\n","  print('layer: ')\n","  print(layer)\n","  print('kernel o parámetros')\n","  print(kernel)\n","  print('bias')\n","  print(bias)\n","  print('total')\n","  total += torch.prod(kernel) + bias\n","  print(total)\n","  print('------')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"whXbDVzRWorm","executionInfo":{"status":"ok","timestamp":1711585177894,"user_tz":180,"elapsed":321,"user":{"displayName":"Marcos Uriel Maillot","userId":"16876029369473785241"}},"outputId":"19224a34-6892-4fcb-b01e-1f568ec3e893"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tamaño imagen ejemplo\n","torch.Size([1, 3, 300, 200])\n","tamaño parcial ANTES de fc\n","torch.Size([1, 64, 34, 21])\n","\n","salida final de la fc\n","torch.Size([1, 10])\n","----------------------------------------\n","----------------------------------------\n","layer: \n","Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n","kernel o parámetros\n","tensor([16,  3,  5,  5])\n","bias\n","tensor([16])\n","total\n","tensor([1216])\n","------\n","layer: \n","Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n","kernel o parámetros\n","tensor([32, 16,  5,  5])\n","bias\n","tensor([32])\n","total\n","tensor([14048])\n","------\n","layer: \n","Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n","kernel o parámetros\n","tensor([64, 32,  5,  5])\n","bias\n","tensor([64])\n","total\n","tensor([65312])\n","------\n","layer: \n","Linear(in_features=45696, out_features=10, bias=True)\n","kernel o parámetros\n","tensor([   10, 45696])\n","bias\n","tensor([10])\n","total\n","tensor([522282])\n","------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HdUsrX4tXl2Q"},"execution_count":null,"outputs":[]}]}
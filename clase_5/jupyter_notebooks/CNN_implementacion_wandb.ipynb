{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wIQ8hjDpdVi"
      },
      "source": [
        "# Importar lo necesario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHQUjDs12DLW"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary # para ver los parametros y tamaños intermedios del modelo\n",
        "from tqdm import tqdm # para graficar la barra de avance\n",
        "import wandb # hacer log en whights and bias\n",
        "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingWarmRestarts # modificar LR conforme se entrena"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "import torchinfo as torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biK7BQSsgEM0",
        "outputId": "ad75b842-42b3-4813-c3db-15ad4d28e03a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeJy8fjPn4wi"
      },
      "source": [
        "#### configuramos el `device` acorde al device disponible\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOV9xybtn4I3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "64b1e4d6-da1d-4eb2-995b-1c7f38342ae3"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### nos vinculamos al weights and biases"
      ],
      "metadata": {
        "id": "IROzIJYLho4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# key de Marcos Uriel Maillot (lelectronfou@gmail.com), cámbiela a su usario una vez finalizada la clase.\n",
        "wandb.login(key=\"d63a15806a812590a5525d000eed0e6d6c57a023\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjEyZuHV97Iw",
        "outputId": "4af6bdd5-3114-42ec-af6e-9768b027d792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlelectronfou\u001b[0m (\u001b[33mmmaillot\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_tH9u082jpZ"
      },
      "source": [
        "\n",
        "#**MNIST data base**\n",
        "# Ejemplo de red neuronal de convolución (CNN)\n",
        "\n",
        "Vamos a usar la base de datos de MNIST ([ver fuente](http://yann.lecun.com/exdb/mnist/)) para entrenar una CNN que identifique números escritos a mano.\n",
        "\n",
        "Para esto necesitamos:\n",
        "\n",
        "\n",
        "1.   Cargar la base de datos.\n",
        "2.   Ver que la base de datos esté ok.\n",
        "3.   Construir nuestra CNN.\n",
        "4. Ver que las dimensiones de la red sean consistentes.\n",
        "4.   Definir funciones necesarias (de entrenamiento, de costo, etc.).\n",
        "5. Entrenar la red.\n",
        "6. Ver que funcione.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nQ-MLk6Do8e"
      },
      "source": [
        "## 1. Cargar base de datos\n",
        "\n",
        "De la documentación, ver:\n",
        "\n",
        "\n",
        "Transformación `torchvision.transforms.ToTensor()`\n",
        "\n",
        "```\n",
        "... Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]...\n",
        "```\n",
        "\n",
        "Transformación `Normalize`\n",
        "\n",
        "```\n",
        "... Normalize a tensor image with mean and standard deviation. ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JvzatGF4e0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ac94b7-1f7e-4db8-c679-943abe6176e7"
      },
      "source": [
        "# primero creamos el dataset\n",
        "train_dataset = torchvision.datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=torchvision.transforms.Compose([\n",
        "                            torchvision.transforms.ToTensor(),#<---------------- escala entre 0 y 1; pasa a tensor; poner canal en 1ra dim\n",
        "                            torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                            ])\n",
        "                      )\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST('../data', train=False,\n",
        "                   transform=torchvision.transforms.Compose([\n",
        "                        torchvision.transforms.ToTensor(), #<------------------- escala entre 0 y 1; pasa a tensor; poner canal en 1ra dim\n",
        "                        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                        ])\n",
        "                     )\n",
        "\n",
        "# ahora el dataloader\n",
        "dataloader = {\n",
        "    'train': torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True),\n",
        "    'test': torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.12MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 132kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.09MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.78MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oikthAE4Dteb"
      },
      "source": [
        "## 2. Ver que la base de datos esté OK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyq2UFIl-Qjy",
        "outputId": "152c7595-f36c-4b31-dd0a-ca66a29f6b33"
      },
      "source": [
        "print(type(dataloader))\n",
        "print(type(dataloader['train']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver imagen and label del dataloader (dataloader -> una herramienta para hacer batches de datasets)\n",
        "train_features, train_labels = next(iter(dataloader['train']))"
      ],
      "metadata": {
        "id": "3dVPXQRch4xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2fs6Qdivs1H",
        "outputId": "ef2ff95f-4a5b-424b-ede1-8d23bb9453ab"
      },
      "source": [
        "# verifico sus dimensiones\n",
        "print(f\"Tamaño del batch de feature (input / imagen): {train_features.size()}\")\n",
        "print(f\"Tamaño del batch del label (clase / etiqueta): {train_labels.size()}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del batch de feature (input / imagen): torch.Size([64, 1, 28, 28])\n",
            "Tamaño del batch del label (clase / etiqueta): torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# tomo 1 imagen para poder visualizarla\n",
        "# y verifico sus dimensiones\n",
        "\n",
        "img = train_features[5]\n",
        "print('tamaño de 1 imagen: ', img.shape)\n",
        "# le QUITO 1 dimension (la del tamaño del batch) para poder graficar\n",
        "img = img.squeeze()\n",
        "print('tamaño de 1 imagen DESPUES de squeeze: ', img.shape)\n",
        "label = train_labels[5]\n",
        "\n",
        "# ploteo esa imagen\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "id": "rfK_dXQdI2C6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "c9f7f094-60b9-4a5b-a8cc-3abb337f33a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamaño de 1 imagen:  torch.Size([1, 28, 28])\n",
            "tamaño de 1 imagen DESPUES de squeeze:  torch.Size([28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAG6VJREFUeJzt3X9sleX9//HXKdJjlfawUtrTSsGCP9jkxyKT2qgVRkPpNgeCGzqTwWJkYHEi80dqpuhc0o0t6lyYmM2ARvEHccA0W51WWzZXICCEkGlHm2rroEWJnFNaKU17ff/g6/l4oAXvwzl9n5bnI7mSnvu+373fXt7pq/c5N1d9zjknAAAGWIp1AwCAcxMBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABPnWTdwst7eXh04cEDp6eny+XzW7QAAPHLOqb29XXl5eUpJ6f8+J+kC6MCBA8rPz7duAwBwllpaWjRmzJh+9yfdW3Dp6enWLQAA4uBMP88TFkBr1qzRxRdfrPPPP1+FhYXasWPHV6rjbTcAGBrO9PM8IQH08ssva+XKlVq1apXee+89TZ06VaWlpTp06FAiTgcAGIxcAkyfPt2Vl5dHXvf09Li8vDxXWVl5xtpQKOQkMRgMBmOQj1AodNqf93G/Azp+/Lh27dqlkpKSyLaUlBSVlJSorq7ulOO7uroUDoejBgBg6It7AH366afq6elRTk5O1PacnBy1traecnxlZaUCgUBk8AQcAJwbzJ+Cq6ioUCgUioyWlhbrlgAAAyDu/w4oKytLw4YNU1tbW9T2trY2BYPBU473+/3y+/3xbgMAkOTifgeUmpqqadOmqbq6OrKtt7dX1dXVKioqivfpAACDVEJWQli5cqUWLVqkb33rW5o+fbqeeOIJdXR06Cc/+UkiTgcAGIQSEkALFy7UJ598ooceekitra365je/qaqqqlMeTAAAnLt8zjln3cSXhcNhBQIB6zYAAGcpFAopIyOj3/3mT8EBAM5NBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwMR51g0A56JXX33Vc43P5/Nc89Of/tRzjSR98sknMdUBXnAHBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwASLkQJfkpLi/Xeym266yXPN/PnzPdfEYsuWLTHVPfvss3HuBDgVd0AAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMsBgp8CWXXXaZ55qXX345AZ2caufOnZ5rWltbE9AJEB/cAQEATBBAAAATcQ+ghx9+WD6fL2pMnDgx3qcBAAxyCfkM6IorrtBbb731fyc5j4+aAADREpIM5513noLBYCK+NQBgiEjIZ0D79+9XXl6exo8fr1tvvVXNzc39HtvV1aVwOBw1AABDX9wDqLCwUOvXr1dVVZWeeuopNTU16brrrlN7e3ufx1dWVioQCERGfn5+vFsCACShuAdQWVmZfvCDH2jKlCkqLS3V3/72Nx05ckSvvPJKn8dXVFQoFApFRktLS7xbAgAkoYQ/HTBy5Ehddtllamho6HO/3++X3+9PdBsAgCST8H8HdPToUTU2Nio3NzfRpwIADCJxD6B77rlHtbW1+vDDD/Xvf/9bN954o4YNG6Zbbrkl3qcCAAxicX8L7uOPP9Ytt9yiw4cPa/To0br22mu1bds2jR49Ot6nAgAMYnEPoJdeeine3xIYMIsXLx6Q8/T09Hiuue222zzX7N2713MNMFBYCw4AYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJhP9BOuBspaWlea753e9+F9O5lixZElOdV3feeafnGhYWxVDDHRAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwASrYSPprVixwnPNHXfcEf9G+nH//fd7rlm7dm0COgEGF+6AAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmGAxUiS9SZMmDdi5WlpaPNf86U9/8lzjnPNcAww13AEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwwWKkGFAjRozwXDN79uwEdNK3AwcOeK757LPPEtCJrfHjx3uuSU1N9VzT2Njouaa7u9tzDZITd0AAABMEEADAhOcA2rp1q2644Qbl5eXJ5/Np8+bNUfudc3rooYeUm5urtLQ0lZSUaP/+/fHqFwAwRHgOoI6ODk2dOlVr1qzpc//q1av15JNPau3atdq+fbsuvPBClZaW6tixY2fdLABg6PD8EEJZWZnKysr63Oec0xNPPKFf/OIXmjt3riTpueeeU05OjjZv3qybb7757LoFAAwZcf0MqKmpSa2trSopKYlsCwQCKiwsVF1dXZ81XV1dCofDUQMAMPTFNYBaW1slSTk5OVHbc3JyIvtOVllZqUAgEBn5+fnxbAkAkKTMn4KrqKhQKBSKjJaWFuuWAAADIK4BFAwGJUltbW1R29va2iL7Tub3+5WRkRE1AABDX1wDqKCgQMFgUNXV1ZFt4XBY27dvV1FRUTxPBQAY5Dw/BXf06FE1NDREXjc1NWnPnj3KzMzU2LFjtWLFCv3qV7/SpZdeqoKCAj344IPKy8vTvHnz4tk3AGCQ8xxAO3fu1MyZMyOvV65cKUlatGiR1q9fr/vuu08dHR1asmSJjhw5omuvvVZVVVU6//zz49c1AGDQ8znnnHUTXxYOhxUIBKzbQII8/fTTnmuWLFniueajjz7yXCNJM2bM8Fzz4YcfxnQuryZOnOi55v7774/pXAsXLvRck5aW5rnm97//veeaFStWeK6BjVAodNrP9c2fggMAnJsIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACY8/zkG4Gzk5OQMyHl27NgRU91ArWx95ZVXeq6pqqryXDN69GjPNQPprrvu8lzzxhtveK75+9//7rkGiccdEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMsRoqYpaWlea4pKChIQCenev/99wfkPJKUkuL997jHH3/cc81ALiy6e/duzzX/+9//PNd873vf81xzxRVXeK5hMdLkxB0QAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEyxGipiNGDHCc82UKVMS0Mmp3n333QE5jyT9+Mc/9lxTXFycgE5OtX///pjqZs6c6bnm6quv9lwTy2KkP/vZzzzXPP30055rJKm9vT2mOnw13AEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwwWKkSHqxLAj54Ycfxr+RQejxxx+PqS4UCnmumTx5ckzn8uq///2v55rOzs4EdIKzxR0QAMAEAQQAMOE5gLZu3aobbrhBeXl58vl82rx5c9T+xYsXy+fzRY05c+bEq18AwBDhOYA6Ojo0depUrVmzpt9j5syZo4MHD0bGiy++eFZNAgCGHs8PIZSVlamsrOy0x/j9fgWDwZibAgAMfQn5DKimpkbZ2dm6/PLLtWzZMh0+fLjfY7u6uhQOh6MGAGDoi3sAzZkzR88995yqq6v1m9/8RrW1tSorK1NPT0+fx1dWVioQCERGfn5+vFsCACShuP87oJtvvjny9eTJkzVlyhRNmDBBNTU1mjVr1inHV1RUaOXKlZHX4XCYEAKAc0DCH8MeP368srKy1NDQ0Od+v9+vjIyMqAEAGPoSHkAff/yxDh8+rNzc3ESfCgAwiHh+C+7o0aNRdzNNTU3as2ePMjMzlZmZqUceeUQLFixQMBhUY2Oj7rvvPl1yySUqLS2Na+MAgMHNcwDt3LlTM2fOjLz+4vObRYsW6amnntLevXv17LPP6siRI8rLy9Ps2bP16KOPyu/3x69rAMCg5zmAZsyYIedcv/vfeOONs2oIONlnn33muSaWBSuTXX9Pkp7OX//615jONXz4cM813//+92M6l1fvv/++55pY5g6Jx1pwAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATcf+T3Dh3dHZ2eq6pr6/3XHPhhRd6ron1z390dXV5rtm4caPnmj//+c+ea1JSvP++eOmll3qukaRHH33Uc811113nuaa3t9dzzebNmz3XIDlxBwQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEi5EiZh0dHZ5rPvjgA881c+fO9Vxz/fXXe66RpH/84x+ea2KZB+ec5xqfz+e55rnnnvNcI0n5+fkx1Xm1Y8cOzzXV1dUJ6AQWuAMCAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgwudiWRUxgcLhsAKBgHUbSJCysjLPNa+99prnmn379nmukaSbbrrJc82hQ4c819TV1Xmu+cY3vuG5ZiDt37/fc821117ruSaW+YaNUCikjIyMfvdzBwQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEi5Ei6f3zn//0XBPLIpeS1NnZ6bmmubnZc83EiRM91wyk7u5uzzU//OEPPdds3rzZcw0GDxYjBQAkJQIIAGDCUwBVVlbqqquuUnp6urKzszVv3jzV19dHHXPs2DGVl5dr1KhRGjFihBYsWKC2tra4Ng0AGPw8BVBtba3Ky8u1bds2vfnmm+ru7tbs2bPV0dEROebuu+/Wa6+9po0bN6q2tlYHDhzQ/Pnz4944AGBwO8/LwVVVVVGv169fr+zsbO3atUvFxcUKhUJ65plntGHDBn3729+WJK1bt05f//rXtW3bNl199dXx6xwAMKid1WdAoVBIkpSZmSlJ2rVrl7q7u1VSUhI5ZuLEiRo7dmy/f4K4q6tL4XA4agAAhr6YA6i3t1crVqzQNddco0mTJkmSWltblZqaqpEjR0Ydm5OTo9bW1j6/T2VlpQKBQGTk5+fH2hIAYBCJOYDKy8u1b98+vfTSS2fVQEVFhUKhUGS0tLSc1fcDAAwOnj4D+sLy5cv1+uuva+vWrRozZkxkezAY1PHjx3XkyJGou6C2tjYFg8E+v5ff75ff74+lDQDAIObpDsg5p+XLl2vTpk16++23VVBQELV/2rRpGj58uKqrqyPb6uvr1dzcrKKiovh0DAAYEjzdAZWXl2vDhg3asmWL0tPTI5/rBAIBpaWlKRAI6LbbbtPKlSuVmZmpjIwM3XnnnSoqKuIJOABAFE8B9NRTT0mSZsyYEbV93bp1Wrx4sSTp8ccfV0pKihYsWKCuri6Vlpbqj3/8Y1yaBQAMHSxGiqQXy93zY489FtO5htpbxc8//3xMdc8884znmpqampjOhaGLxUgBAEmJAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCC1bAxJI0aNSqmultvvdVzzQMPPOC5Jicnx3PNq6++6rlm2bJlnmsk6ZNPPompDvgyVsMGACQlAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJliMFACQECxGCgBISgQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMeAqgyspKXXXVVUpPT1d2drbmzZun+vr6qGNmzJghn88XNZYuXRrXpgEAg5+nAKqtrVV5ebm2bdumN998U93d3Zo9e7Y6Ojqijrv99tt18ODByFi9enVcmwYADH7neTm4qqoq6vX69euVnZ2tXbt2qbi4OLL9ggsuUDAYjE+HAIAh6aw+AwqFQpKkzMzMqO0vvPCCsrKyNGnSJFVUVKizs7Pf79HV1aVwOBw1AADnABejnp4e993vftddc801UduffvppV1VV5fbu3euef/55d9FFF7kbb7yx3++zatUqJ4nBYDAYQ2yEQqHT5kjMAbR06VI3btw419LSctrjqqurnSTX0NDQ5/5jx465UCgUGS0tLeaTxmAwGIyzH2cKIE+fAX1h+fLlev3117V161aNGTPmtMcWFhZKkhoaGjRhwoRT9vv9fvn9/ljaAAAMYp4CyDmnO++8U5s2bVJNTY0KCgrOWLNnzx5JUm5ubkwNAgCGJk8BVF5erg0bNmjLli1KT09Xa2urJCkQCCgtLU2NjY3asGGDvvOd72jUqFHau3ev7r77bhUXF2vKlCkJ+Q8AAAxSXj73UT/v861bt84551xzc7MrLi52mZmZzu/3u0suucTde++9Z3wf8MtCoZD5+5YMBoPBOPtxpp/9vv8fLEkjHA4rEAhYtwEAOEuhUEgZGRn97mctOACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACAiaQLIOecdQsAgDg408/zpAug9vZ26xYAAHFwpp/nPpdktxy9vb06cOCA0tPT5fP5ovaFw2Hl5+erpaVFGRkZRh3aYx5OYB5OYB5OYB5OSIZ5cM6pvb1deXl5Sknp/z7nvAHs6StJSUnRmDFjTntMRkbGOX2BfYF5OIF5OIF5OIF5OMF6HgKBwBmPSbq34AAA5wYCCABgYlAFkN/v16pVq+T3+61bMcU8nMA8nMA8nMA8nDCY5iHpHkIAAJwbBtUdEABg6CCAAAAmCCAAgAkCCABgYtAE0Jo1a3TxxRfr/PPPV2FhoXbs2GHd0oB7+OGH5fP5osbEiROt20q4rVu36oYbblBeXp58Pp82b94ctd85p4ceeki5ublKS0tTSUmJ9u/fb9NsAp1pHhYvXnzK9TFnzhybZhOksrJSV111ldLT05Wdna158+apvr4+6phjx46pvLxco0aN0ogRI7RgwQK1tbUZdZwYX2UeZsyYccr1sHTpUqOO+zYoAujll1/WypUrtWrVKr333nuaOnWqSktLdejQIevWBtwVV1yhgwcPRsa//vUv65YSrqOjQ1OnTtWaNWv63L969Wo9+eSTWrt2rbZv364LL7xQpaWlOnbs2AB3mlhnmgdJmjNnTtT18eKLLw5gh4lXW1ur8vJybdu2TW+++aa6u7s1e/ZsdXR0RI65++679dprr2njxo2qra3VgQMHNH/+fMOu4++rzIMk3X777VHXw+rVq4067ocbBKZPn+7Ky8sjr3t6elxeXp6rrKw07GrgrVq1yk2dOtW6DVOS3KZNmyKve3t7XTAYdL/97W8j244cOeL8fr978cUXDTocGCfPg3POLVq0yM2dO9ekHyuHDh1yklxtba1z7sT/++HDh7uNGzdGjnn//fedJFdXV2fVZsKdPA/OOXf99de7u+66y66pryDp74COHz+uXbt2qaSkJLItJSVFJSUlqqurM+zMxv79+5WXl6fx48fr1ltvVXNzs3VLppqamtTa2hp1fQQCARUWFp6T10dNTY2ys7N1+eWXa9myZTp8+LB1SwkVCoUkSZmZmZKkXbt2qbu7O+p6mDhxosaOHTukr4eT5+ELL7zwgrKysjRp0iRVVFSos7PTor1+Jd1ipCf79NNP1dPTo5ycnKjtOTk5+uCDD4y6slFYWKj169fr8ssv18GDB/XII4/ouuuu0759+5Senm7dnonW1lZJ6vP6+GLfuWLOnDmaP3++CgoK1NjYqAceeEBlZWWqq6vTsGHDrNuLu97eXq1YsULXXHONJk2aJOnE9ZCamqqRI0dGHTuUr4e+5kGSfvSjH2ncuHHKy8vT3r17df/996u+vl5/+ctfDLuNlvQBhP9TVlYW+XrKlCkqLCzUuHHj9Morr+i2224z7AzJ4Oabb458PXnyZE2ZMkUTJkxQTU2NZs2aZdhZYpSXl2vfvn3nxOegp9PfPCxZsiTy9eTJk5Wbm6tZs2apsbFREyZMGOg2+5T0b8FlZWVp2LBhpzzF0tbWpmAwaNRVchg5cqQuu+wyNTQ0WLdi5otrgOvjVOPHj1dWVtaQvD6WL1+u119/Xe+8807Un28JBoM6fvy4jhw5EnX8UL0e+puHvhQWFkpSUl0PSR9AqampmjZtmqqrqyPbent7VV1draKiIsPO7B09elSNjY3Kzc21bsVMQUGBgsFg1PURDoe1ffv2c/76+Pjjj3X48OEhdX0457R8+XJt2rRJb7/9tgoKCqL2T5s2TcOHD4+6Hurr69Xc3DykroczzUNf9uzZI0nJdT1YPwXxVbz00kvO7/e79evXu//85z9uyZIlbuTIka61tdW6tQH185//3NXU1Limpib37rvvupKSEpeVleUOHTpk3VpCtbe3u927d7vdu3c7Se6xxx5zu3fvdh999JFzzrlf//rXbuTIkW7Lli1u7969bu7cua6goMB9/vnnxp3H1+nmob293d1zzz2urq7ONTU1ubfeestdeeWV7tJLL3XHjh2zbj1uli1b5gKBgKupqXEHDx6MjM7OzsgxS5cudWPHjnVvv/2227lzpysqKnJFRUWGXcffmeahoaHB/fKXv3Q7d+50TU1NbsuWLW78+PGuuLjYuPNogyKAnHPuD3/4gxs7dqxLTU1106dPd9u2bbNuacAtXLjQ5ebmutTUVHfRRRe5hQsXuoaGBuu2Eu6dd95xkk4ZixYtcs6deBT7wQcfdDk5Oc7v97tZs2a5+vp626YT4HTz0NnZ6WbPnu1Gjx7thg8f7saNG+duv/32IfdLWl///ZLcunXrIsd8/vnn7o477nBf+9rX3AUXXOBuvPFGd/DgQbumE+BM89Dc3OyKi4tdZmam8/v97pJLLnH33nuvC4VCto2fhD/HAAAwkfSfAQEAhiYCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAm/h+Kk9KvgWJhswAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('pixel [0,0]: ',img[0][0])\n",
        "print('pixel maximo: ', torch.max(img))\n",
        "print('pixel minimo: ', torch.min(img))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFnOkkTgYTHV",
        "outputId": "972b1b92-8aab-479d-f99b-74fdac04d43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pixel [0,0]:  tensor(-0.4242)\n",
            "pixel maximo:  tensor(2.8088)\n",
            "pixel minimo:  tensor(-0.4242)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Construyo mi CNN"
      ],
      "metadata": {
        "id": "hLDYgFptiqPd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY0TN4erDxRd"
      },
      "source": [
        "#### Bloque de convolución\n",
        "\n",
        "defino primero un \"bloque\" de una capa CNN\n",
        "construido con los bloques funcionales vistos en clase\n",
        "\n",
        "argumentos a pasar a la función:\n",
        "\n",
        "  - `c_in`:   canales (kernels) de entrada\n",
        "  - `c_out`:  canales (kernels) de salida\n",
        "  - `k`:      tamaño del kernel kxk\n",
        "  - `p`:      tamaño del padding de la convolución\n",
        "  - `s`:      stride de la convolución\n",
        "  - `pk`:     tamaño del kernel del pooling\n",
        "\n",
        "\n",
        "la función pooling se elige directamente dentro del bloque!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bloque de convolución para emplear en mi red\n",
        "\n",
        "def conv_block(c_in, c_out, k=3, p='same', s=1, pk=2):\n",
        "    return torch.nn.Sequential(                               # el módulo Sequential se engarga de hacer el forward de todo lo que tiene dentro.\n",
        "        torch.nn.Conv2d(c_in, c_out, k, padding=p, stride=s), # conv\n",
        "        torch.nn.Tanh(),                                      # activation\n",
        "        torch.nn.MaxPool2d(pk)                                # pooling\n",
        "    )\n"
      ],
      "metadata": {
        "id": "qMxa2DAsim9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Red convolucional (modelo)\n",
        "\n",
        "\n",
        "Ahora SI construyo mi red... usando la clase CNN de pytorch\n",
        "revisar muy bien las dimensiones a emplear en cada capa y\n",
        "tener presente la reducción de las dimensiones.\n",
        "\n",
        "En la útlima capa fully conected `fc`, hacer bien el cálculo final del\n",
        "tamaño del array que se obtiene siguiendo la formula vista en la teoria\n",
        "tanto para la capa conv como para la capa pooling."
      ],
      "metadata": {
        "id": "PgPfrY8VivH8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrO5gfEL3KRC"
      },
      "source": [
        "class CNN(torch.nn.Module):\n",
        "  def __init__(self, n_channels=1, n_outputs=10):\n",
        "    super().__init__()\n",
        "    self.conv1 = conv_block(c_in = n_channels, c_out = 16, k=5, p='same', s=1, pk=2)\n",
        "    self.conv1_out = None\n",
        "    self.drop = torch.nn.Dropout2d(p=0.7, inplace=False)\n",
        "    #self.conv2 = conv_block(c_in = 4, c_out = 8, k=3, p='same', s=1, pk=2)\n",
        "    #self.conv2_out = None\n",
        "    #self.conv3 = conv_block(c_in = 8, c_out = 16, k=3, p='same', s=1, pk=2)\n",
        "    self.fc = torch.nn.Linear(16*14*14, n_outputs) # verificar la dim de la salida para calcular el tamaño de la fully conected!!\n",
        "\n",
        "\n",
        "    print('Red creada')\n",
        "    print('arquitectura:')\n",
        "    print(self)\n",
        "\n",
        "    # Me fijo en el número de capas\n",
        "    i=0\n",
        "    for layer in self.children():\n",
        "        i=i+1\n",
        "    print('Número total de capas de CNN (conv+act+polling) + finales : ', i)\n",
        "\n",
        "    # Me fijo en el número de parámetros entrenables\n",
        "    pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "    print('Número total de parámetros a entrenar: ', pytorch_total_params)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print('input shape: ', x.shape)\n",
        "    self.conv1_out = self.drop(self.conv1(x))\n",
        "    #self.conv2_out = self.drop(self.conv2(self.conv1_out))\n",
        "    #y = self.conv3(self.conv2_out)\n",
        "    y = self.conv1_out\n",
        "    y = y.flatten(start_dim=1)\n",
        "    #print(y.shape)\n",
        "    y = self.fc(y)\n",
        "    return y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6DoGaP31md",
        "outputId": "92b13b78-5769-430d-e484-57e59c0f6b1a"
      },
      "source": [
        "# instancio modelo\n",
        "model = CNN()\n",
        "\n",
        "# armo config (OJO!! NO ESTÁ AUTOMATIZADO!)\n",
        "model_config = {\n",
        "        \"num_layers\": 2,\n",
        "        \"kernel_size\": 5,\n",
        "        \"dropout\": 0.7,\n",
        "        \"n_channels\": [16],\n",
        "        \"architecture\": model.__class__.__name__\n",
        "  }\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red creada\n",
            "arquitectura:\n",
            "CNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "    (1): Tanh()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (drop): Dropout2d(p=0.7, inplace=False)\n",
            "  (fc): Linear(in_features=3136, out_features=10, bias=True)\n",
            ")\n",
            "Número total de capas de CNN (conv+act+polling) + finales :  3\n",
            "Número total de parámetros a entrenar:  31786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYgRNa9M8cjI",
        "outputId": "421b44dd-c7f6-4d28-b6d5-6137942eeb1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_layers': 4,\n",
              " 'kernel_size': 7,\n",
              " 'dropout': 0.7,\n",
              " 'n_channels': [4, 8, 16],\n",
              " 'architecture': 'CNN',\n",
              " 'optimizer': 'Adam'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4tEn-XqHVZ7"
      },
      "source": [
        "## 4. Veamos que las dimensiones sean consistentes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(model, input_size=( 12, 1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q44d3Ftwokla",
        "outputId": "d3456984-d520-4aac-eafe-b3df5981b1a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "CNN                                      [12, 10]                  --\n",
              "├─Sequential: 1-1                        [12, 4, 14, 14]           --\n",
              "│    └─Conv2d: 2-1                       [12, 4, 28, 28]           200\n",
              "│    └─Tanh: 2-2                         [12, 4, 28, 28]           --\n",
              "│    └─MaxPool2d: 2-3                    [12, 4, 14, 14]           --\n",
              "├─Dropout2d: 1-2                         [12, 4, 14, 14]           --\n",
              "├─Sequential: 1-3                        [12, 8, 7, 7]             --\n",
              "│    └─Conv2d: 2-4                       [12, 8, 14, 14]           1,576\n",
              "│    └─Tanh: 2-5                         [12, 8, 14, 14]           --\n",
              "│    └─MaxPool2d: 2-6                    [12, 8, 7, 7]             --\n",
              "├─Dropout2d: 1-4                         [12, 8, 7, 7]             --\n",
              "├─Sequential: 1-5                        [12, 16, 3, 3]            --\n",
              "│    └─Conv2d: 2-7                       [12, 16, 7, 7]            6,288\n",
              "│    └─Tanh: 2-8                         [12, 16, 7, 7]            --\n",
              "│    └─MaxPool2d: 2-9                    [12, 16, 3, 3]            --\n",
              "├─Linear: 1-6                            [12, 10]                  1,450\n",
              "==========================================================================================\n",
              "Total params: 9,514\n",
              "Trainable params: 9,514\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 9.30\n",
              "==========================================================================================\n",
              "Input size (MB): 0.04\n",
              "Forward/backward pass size (MB): 0.53\n",
              "Params size (MB): 0.04\n",
              "Estimated Total Size (MB): 0.60\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoB3GvDtGUgY"
      },
      "source": [
        "## 5. Armo las funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_wandb(\n",
        "        model,\n",
        "        optimizer,\n",
        "        train_loader,\n",
        "        eval_loader,\n",
        "        loss_module,\n",
        "        config = model_config,\n",
        "        scheduler_type=\"step\",\n",
        "        patience=3,\n",
        "        patience_factor=0.01,\n",
        "        num_epochs=3\n",
        "):\n",
        "    # Initialize Weights & Biases\n",
        "    wandb.init(project=\"CEIA-Co18 - CNN-mnist\", config=model_config)\n",
        "\n",
        "    ## Set device\n",
        "    model.to(device)\n",
        "\n",
        "    # Set metric for callbacks\n",
        "    best_eval = 0\n",
        "    pt_epoch = 0\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Epoch Progress\"):\n",
        "        ### Training Phase ###\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_samples = 0\n",
        "\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "\n",
        "            X, y = data\n",
        "            # Move input data to device (if using GPU)\n",
        "            data_inputs = X.to(device)\n",
        "            data_labels = y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)  # Ensure shape consistency\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_module(preds, data_labels)\n",
        "\n",
        "            # Zero gradients, backpropagate, and update weights\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track loss for this batch\n",
        "            batch_loss = loss.item()\n",
        "            train_loss += batch_loss\n",
        "\n",
        "            # Compute accuracy (assuming classification task)\n",
        "            if preds.ndim == 2:  # Softmax case\n",
        "                preds_classes = preds.argmax(dim=1)\n",
        "            else:  # Sigmoid case (binary classification)\n",
        "                preds_classes = (preds > 0.5).long()\n",
        "\n",
        "            train_correct += (preds_classes == data_labels).sum().item()\n",
        "            train_samples += data_labels.size(0)\n",
        "\n",
        "            # Log batch-wise metrics\n",
        "            wandb.log({\n",
        "                \"batch_loss\": batch_loss,\n",
        "                \"batch_step\": epoch * len(train_loader) + batch_idx,\n",
        "                \"batch_lr\": optimizer.param_groups[0]['lr'],\n",
        "            })\n",
        "\n",
        "            # Step-level callback\n",
        "            if scheduler_type==\"step\":\n",
        "                scheduler.step()\n",
        "\n",
        "        # Compute training metrics\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy = train_correct / train_samples\n",
        "\n",
        "        ### Evaluation Phase ###\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "        eval_correct = 0\n",
        "        eval_samples = 0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient tracking\n",
        "            for data in eval_loader:\n",
        "\n",
        "                X, y = data\n",
        "                data_inputs = X.to(device)\n",
        "                data_labels = y.to(device)\n",
        "\n",
        "                preds = model(data_inputs)\n",
        "                preds = preds.squeeze(dim=1)\n",
        "\n",
        "                loss = loss_module(preds, data_labels)\n",
        "                eval_loss += loss.item()\n",
        "\n",
        "                if preds.ndim == 2:\n",
        "                    preds_classes = preds.argmax(dim=1)\n",
        "                else:\n",
        "                    preds_classes = (preds > 0.5).long()\n",
        "\n",
        "                eval_correct += (preds_classes == data_labels).sum().item()\n",
        "                eval_samples += data_labels.size(0)\n",
        "\n",
        "        # Compute evaluation metrics\n",
        "        eval_loss /= len(eval_loader)\n",
        "        eval_accuracy = eval_correct / eval_samples\n",
        "\n",
        "        # Log epoch-level metrics\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_accuracy,\n",
        "            \"eval_loss\": eval_loss,\n",
        "            \"eval_accuracy\": eval_accuracy,\n",
        "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Train Acc: {train_accuracy:.4f} | Eval Loss: {eval_loss:.4f} - Eval Acc: {eval_accuracy:.4f}\")\n",
        "\n",
        "        # Callbacks\n",
        "        ## Learning rate scheduler\n",
        "        if scheduler_type==\"epoch\":\n",
        "            scheduler.step()\n",
        "        ## Early stopping\n",
        "        if eval_accuracy>=best_eval*(1+patience_factor):\n",
        "            best_eval = eval_accuracy\n",
        "            pt_epoch = 0\n",
        "        else:\n",
        "            pt_epoch += 1\n",
        "            if pt_epoch>=patience:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs} - Training interrupted due to early stopping condition.\")\n",
        "                wandb.finish()\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs} - Current epochs without validation metric improvement {pt_epoch}. {patience-pt_epoch} remaining before stopping.\")\n",
        "\n",
        "    # Finish W&B run\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "9w-WYfZd-Yvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# definimo optimizer y la función de pérdida\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "# paso al config el optimizador\n",
        "model_config[\"optimizer\"] = optimizer.__class__.__name__\n",
        "\n",
        "# defino función de perdida\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Scheduler\n",
        "scheduler_type = \"step\"\n",
        "steps_per_epoch = len(dataloader['train'])\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=steps_per_epoch, T_mult=2, eta_min=1e-5)\n",
        "#scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-5, max_lr=1e-2, step_size_up=steps_per_epoch, mode=\"triangular2\")\n",
        "\n",
        "## Si queremos aplicar cambios al learning rate al final de cada epoch, usamos como ejemplo otro como este:\n",
        "#scheduler_type = \"epoch\"\n",
        "#scheduler = ExponentialLR(optimizer, gamma=0.99) # https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "\n"
      ],
      "metadata": {
        "id": "hpphYbIC_LQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Entrenamiento WandB"
      ],
      "metadata": {
        "id": "0cW5H0nX99DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# corremos la función de entrenamiento con todos los argumentos correspondientes\n",
        "train_model_wandb(model, optimizer, dataloader['train'], dataloader['test'], loss_fn, scheduler_type=scheduler_type, num_epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "id": "47ad8Ydj-ePT",
        "outputId": "3654edeb-e1db-45db-8bef-eaa1b5a450e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250402_004427-n54utip8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mmaillot/CEIA-Co18%20-%20CNN-mnist/runs/n54utip8' target=\"_blank\">golden-flower-30</a></strong> to <a href='https://wandb.ai/mmaillot/CEIA-Co18%20-%20CNN-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mmaillot/CEIA-Co18%20-%20CNN-mnist' target=\"_blank\">https://wandb.ai/mmaillot/CEIA-Co18%20-%20CNN-mnist</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mmaillot/CEIA-Co18%20-%20CNN-mnist/runs/n54utip8' target=\"_blank\">https://wandb.ai/mmaillot/CEIA-Co18%20-%20CNN-mnist/runs/n54utip8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch Progress:   7%|▋         | 1/15 [00:17<04:05, 17.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 - Train Loss: 0.8723 - Train Acc: 0.7919 | Eval Loss: 0.4666 - Eval Acc: 0.8903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch Progress:  13%|█▎        | 2/15 [00:32<03:32, 16.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/15 - Train Loss: 0.4916 - Train Acc: 0.8788 | Eval Loss: 0.3127 - Eval Acc: 0.9145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch Progress:  20%|██        | 3/15 [00:49<03:17, 16.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/15 - Train Loss: 0.4166 - Train Acc: 0.8946 | Eval Loss: 0.2951 - Eval Acc: 0.9181\n",
            "Epoch 3/15 - Current epochs without validation metric improvement 1. 2 remaining before stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch Progress:  27%|██▋       | 4/15 [01:05<02:57, 16.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/15 - Train Loss: 0.3855 - Train Acc: 0.9006 | Eval Loss: 0.2599 - Eval Acc: 0.9257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch Progress:  33%|███▎      | 5/15 [01:20<02:38, 15.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/15 - Train Loss: 0.3529 - Train Acc: 0.9093 | Eval Loss: 0.2443 - Eval Acc: 0.9301\n",
            "Epoch 5/15 - Current epochs without validation metric improvement 1. 2 remaining before stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch Progress:  40%|████      | 6/15 [01:35<02:21, 15.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/15 - Train Loss: 0.3363 - Train Acc: 0.9137 | Eval Loss: 0.2339 - Eval Acc: 0.9335\n",
            "Epoch 6/15 - Current epochs without validation metric improvement 2. 1 remaining before stopping.\n",
            "Epoch 7/15 - Train Loss: 0.3308 - Train Acc: 0.9162 | Eval Loss: 0.2311 - Eval Acc: 0.9337\n",
            "Epoch 7/15 - Training interrupted due to early stopping condition.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>█▆▅▅▆▅▃▄▅▄▄▄▃▄▃▂▄▃▄▄▄▂▂▃▃▃▂▃▂▄▂▅▃▄▃▃▂▃▁▂</td></tr><tr><td>batch_lr</td><td>█▆▅▃▁█▇▇▆▅▄▄▄▃▃▁▁▁████▇▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁▁</td></tr><tr><td>batch_step</td><td>▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>eval_accuracy</td><td>▁▅▅▇▇██</td></tr><tr><td>eval_loss</td><td>█▃▃▂▁▁▁</td></tr><tr><td>learning_rate</td><td>█▄█▇▄▁█</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇███</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.33766</td></tr><tr><td>batch_lr</td><td>1e-05</td></tr><tr><td>batch_step</td><td>6565</td></tr><tr><td>epoch</td><td>6</td></tr><tr><td>eval_accuracy</td><td>0.9337</td></tr><tr><td>eval_loss</td><td>0.23111</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train_accuracy</td><td>0.91623</td></tr><tr><td>train_loss</td><td>0.33085</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">golden-flower-30</strong> at: <a href='https://wandb.ai/mmaillot/CEIA-Co18%20-%20CNN-mnist/runs/n54utip8' target=\"_blank\">https://wandb.ai/mmaillot/CEIA-Co18%20-%20CNN-mnist/runs/n54utip8</a><br> View project at: <a href='https://wandb.ai/mmaillot/CEIA-Co18%20-%20CNN-mnist' target=\"_blank\">https://wandb.ai/mmaillot/CEIA-Co18%20-%20CNN-mnist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250402_004427-n54utip8/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch Progress:  40%|████      | 6/15 [01:54<02:51, 19.04s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHcr6aDtHNfc"
      },
      "source": [
        "## 7. Vemos que funcione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "k_jFvJ603PC-",
        "outputId": "2961c411-8eeb-4eb8-f902-cc083df20d3b"
      },
      "source": [
        "# corremos 1 dato, a ver como lo clasifica...\n",
        "# generamos un batch del dataloader\n",
        "test_features, test_labels = next(iter(dataloader['test']))\n",
        "\n",
        "# item a usar k\n",
        "k = 14\n",
        "\n",
        "# verifico las dimensiones y los valores que toma algun pixel.\n",
        "samp_img = test_features[k]\n",
        "print(samp_img.shape)\n",
        "print(samp_img[0][0][0])\n",
        "print(torch.max(samp_img))\n",
        "print(torch.min(samp_img))\n",
        "\n",
        "# ploteo la imagen\n",
        "plt.imshow(samp_img.squeeze(), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# preparo para pasarla a la red (model) asi predice.\n",
        "samp_imp = samp_img.unsqueeze(0) # agrego la batch dim\n",
        "samp_img = samp_img.unsqueeze(0).to(device)\n",
        "print('Tamaño imagen de entrada a red: ', samp_img.shape)\n",
        "\n",
        "# la paso al modelo\n",
        "model.to(device)\n",
        "model.eval()\n",
        "y_hat = model(samp_img)\n",
        "print('Predición del modelo:')\n",
        "print(y_hat.detach())\n",
        "print()\n",
        "print('softmax de predicción:')\n",
        "print(torch.nn.functional.softmax(y_hat, dim=1).detach())\n",
        "print()\n",
        "print(f'El numero es un: ', torch.argmax(y_hat, axis=1).item())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "tensor(-0.4242)\n",
            "tensor(2.8215)\n",
            "tensor(-0.4242)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño imagen de entrada a red:  torch.Size([1, 1, 28, 28])\n",
            "Predición del modelo:\n",
            "tensor([[-2.2373,  5.4469, -0.6796,  0.2946, -0.8774, -0.0928, -0.0867, -0.0913,\n",
            "          0.2713, -0.3452]], device='cuda:0')\n",
            "\n",
            "softmax de predicción:\n",
            "tensor([[4.4632e-04, 9.7018e-01, 2.1191e-03, 5.6137e-03, 1.7388e-03, 3.8108e-03,\n",
            "         3.8338e-03, 3.8165e-03, 5.4846e-03, 2.9607e-03]], device='cuda:0')\n",
            "\n",
            "El numero es un:  1\n"
          ]
        }
      ]
    }
  ]
}
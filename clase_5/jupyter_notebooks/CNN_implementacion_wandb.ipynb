{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wIQ8hjDpdVi"
      },
      "source": [
        "# Importar lo necesario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uHQUjDs12DLW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary # para ver los parametros y tamaños intermedios del modelo\n",
        "from tqdm import tqdm # para graficar la barra de avance\n",
        "import wandb # hacer log en whights and bias\n",
        "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingWarmRestarts # modificar LR conforme se entrena"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biK7BQSsgEM0",
        "outputId": "ad75b842-42b3-4813-c3db-15ad4d28e03a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchinfo in c:\\uba\\uba_venv\\uba_venv\\lib\\site-packages (1.8.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo\n",
        "import torchinfo as torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeJy8fjPn4wi"
      },
      "source": [
        "#### configuramos el `device` acorde al device disponible\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lOV9xybtn4I3",
        "outputId": "64b1e4d6-da1d-4eb2-995b-1c7f38342ae3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IROzIJYLho4P"
      },
      "source": [
        "#### nos vinculamos al weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjEyZuHV97Iw",
        "outputId": "4af6bdd5-3114-42ec-af6e-9768b027d792"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\CIDIEE_NN\\.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlelectronfou\u001b[0m (\u001b[33mmmaillot\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# key de Marcos Uriel Maillot (lelectronfou@gmail.com), cámbiela a su usario una vez finalizada la clase.\n",
        "wandb.login(key=\"d63a15806a812590a5525d000eed0e6d6c57a023\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_tH9u082jpZ"
      },
      "source": [
        "\n",
        "#**MNIST data base**\n",
        "# Ejemplo de red neuronal de convolución (CNN)\n",
        "\n",
        "Vamos a usar la base de datos de MNIST ([ver fuente](http://yann.lecun.com/exdb/mnist/)) para entrenar una CNN que identifique números escritos a mano.\n",
        "\n",
        "Para esto necesitamos:\n",
        "\n",
        "\n",
        "1.   Cargar la base de datos.\n",
        "2.   Ver que la base de datos esté ok.\n",
        "3.   Construir nuestra CNN.\n",
        "4. Ver que las dimensiones de la red sean consistentes.\n",
        "4.   Definir funciones necesarias (de entrenamiento, de costo, etc.).\n",
        "5. Entrenar la red.\n",
        "6. Ver que funcione.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nQ-MLk6Do8e"
      },
      "source": [
        "## 1. Cargar base de datos\n",
        "\n",
        "De la documentación, ver:\n",
        "\n",
        "\n",
        "Transformación `torchvision.transforms.ToTensor()`\n",
        "\n",
        "```\n",
        "... Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]...\n",
        "```\n",
        "\n",
        "Transformación `Normalize`\n",
        "\n",
        "```\n",
        "... Normalize a tensor image with mean and standard deviation. ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JvzatGF4e0W",
        "outputId": "e0ac94b7-1f7e-4db8-c679-943abe6176e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [01:14<00:00, 133782.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 103467.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:12<00:00, 127065.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# primero creamos el dataset\n",
        "train_dataset = torchvision.datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=torchvision.transforms.Compose([\n",
        "                            torchvision.transforms.ToTensor(),#<---------------- escala entre 0 y 1; pasa a tensor; poner canal en 1ra dim\n",
        "                            torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                            ])\n",
        "                      )\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST('../data', train=False,\n",
        "                   transform=torchvision.transforms.Compose([\n",
        "                        torchvision.transforms.ToTensor(), #<------------------- escala entre 0 y 1; pasa a tensor; poner canal en 1ra dim\n",
        "                        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                        ])\n",
        "                     )\n",
        "\n",
        "# ahora el dataloader\n",
        "dataloader = {\n",
        "    'train': torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True),\n",
        "    'test': torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, pin_memory=True)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oikthAE4Dteb"
      },
      "source": [
        "## 2. Ver que la base de datos esté OK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyq2UFIl-Qjy",
        "outputId": "152c7595-f36c-4b31-dd0a-ca66a29f6b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ]
        }
      ],
      "source": [
        "print(type(dataloader))\n",
        "print(type(dataloader['train']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dVPXQRch4xV"
      },
      "outputs": [],
      "source": [
        "# Ver imagen and label del dataloader (dataloader -> una herramienta para hacer batches de datasets)\n",
        "train_features, train_labels = next(iter(dataloader['train']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2fs6Qdivs1H",
        "outputId": "ef2ff95f-4a5b-424b-ede1-8d23bb9453ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del batch de feature (input / imagen): torch.Size([64, 1, 28, 28])\n",
            "Tamaño del batch del label (clase / etiqueta): torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# verifico sus dimensiones\n",
        "print(f\"Tamaño del batch de feature (input / imagen): {train_features.size()}\")\n",
        "print(f\"Tamaño del batch del label (clase / etiqueta): {train_labels.size()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "rfK_dXQdI2C6",
        "outputId": "c9f7f094-60b9-4a5b-a8cc-3abb337f33a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tamaño de 1 imagen:  torch.Size([1, 28, 28])\n",
            "tamaño de 1 imagen DESPUES de squeeze:  torch.Size([28, 28])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label: 6\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# tomo 1 imagen para poder visualizarla\n",
        "# y verifico sus dimensiones\n",
        "\n",
        "img = train_features[5]\n",
        "print('tamaño de 1 imagen: ', img.shape)\n",
        "# le QUITO 1 dimension (la del tamaño del batch) para poder graficar\n",
        "img = img.squeeze()\n",
        "print('tamaño de 1 imagen DESPUES de squeeze: ', img.shape)\n",
        "label = train_labels[5]\n",
        "\n",
        "# ploteo esa imagen\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFnOkkTgYTHV",
        "outputId": "972b1b92-8aab-479d-f99b-74fdac04d43a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pixel [0,0]:  tensor(-0.4242)\n",
            "pixel maximo:  tensor(2.8088)\n",
            "pixel minimo:  tensor(-0.4242)\n"
          ]
        }
      ],
      "source": [
        "print('pixel [0,0]: ',img[0][0])\n",
        "print('pixel maximo: ', torch.max(img))\n",
        "print('pixel minimo: ', torch.min(img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLDYgFptiqPd"
      },
      "source": [
        "## 3. Construyo mi CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY0TN4erDxRd"
      },
      "source": [
        "#### Bloque de convolución\n",
        "\n",
        "defino primero un \"bloque\" de una capa CNN\n",
        "construido con los bloques funcionales vistos en clase\n",
        "\n",
        "argumentos a pasar a la función:\n",
        "\n",
        "  - `c_in`:   canales (kernels) de entrada\n",
        "  - `c_out`:  canales (kernels) de salida\n",
        "  - `k`:      tamaño del kernel kxk\n",
        "  - `p`:      tamaño del padding de la convolución\n",
        "  - `s`:      stride de la convolución\n",
        "  - `pk`:     tamaño del kernel del pooling\n",
        "\n",
        "\n",
        "la función pooling se elige directamente dentro del bloque!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qMxa2DAsim9t"
      },
      "outputs": [],
      "source": [
        "# bloque de convolución para emplear en mi red\n",
        "\n",
        "def conv_block(c_in, c_out, k=3, p='same', s=1, pk=2):\n",
        "    return torch.nn.Sequential(                               # el módulo Sequential se engarga de hacer el forward de todo lo que tiene dentro.\n",
        "        torch.nn.Conv2d(c_in, c_out, k, padding=p, stride=s), # conv\n",
        "        torch.nn.Tanh(),                                      # activation\n",
        "        torch.nn.MaxPool2d(pk)                                # pooling\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgPfrY8VivH8"
      },
      "source": [
        "### Red convolucional (modelo)\n",
        "\n",
        "\n",
        "Ahora SI construyo mi red... usando la clase CNN de pytorch\n",
        "revisar muy bien las dimensiones a emplear en cada capa y\n",
        "tener presente la reducción de las dimensiones.\n",
        "\n",
        "En la útlima capa fully conected `fc`, hacer bien el cálculo final del\n",
        "tamaño del array que se obtiene siguiendo la formula vista en la teoria\n",
        "tanto para la capa conv como para la capa pooling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "JrO5gfEL3KRC"
      },
      "outputs": [],
      "source": [
        "class CNN(torch.nn.Module):\n",
        "  def __init__(self, n_channels=1, n_outputs=10):\n",
        "    super().__init__()\n",
        "    self.conv1 = conv_block(c_in = n_channels, c_out = 16, k=9, p='same', s=1, pk=2)\n",
        "    self.conv1_out = None\n",
        "    self.drop = torch.nn.Dropout2d(p=0.7, inplace=False)\n",
        "    self.conv2 = conv_block(c_in = 16, c_out = 16, k=9, p='same', s=1, pk=2)\n",
        "    self.conv2_out = None\n",
        "    self.conv3 = conv_block(c_in = 16, c_out = 16, k=9, p='same', s=1, pk=2)\n",
        "    self.fc = torch.nn.Linear(16*3*3, n_outputs) # verificar la dim de la salida para calcular el tamaño de la fully conected!!\n",
        "\n",
        "\n",
        "    print('Red creada')\n",
        "    print('arquitectura:')\n",
        "    print(self)\n",
        "\n",
        "    # Me fijo en el número de capas\n",
        "    i=0\n",
        "    for layer in self.children():\n",
        "        i=i+1\n",
        "    print('Número total de capas de CNN (conv+act+polling) + finales : ', i)\n",
        "\n",
        "    # Me fijo en el número de parámetros entrenables\n",
        "    pytorch_total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "    print('Número total de parámetros a entrenar: ', pytorch_total_params)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print('input shape: ', x.shape)\n",
        "    self.conv1_out = self.drop(self.conv1(x))\n",
        "    self.conv2_out = self.drop(self.conv2(self.conv1_out))\n",
        "    self.conv3_out = self.conv3(self.conv2_out)\n",
        "    y = self.conv3_out\n",
        "    y = y.flatten(start_dim=1)\n",
        "    #print(y.shape)\n",
        "    y = self.fc(y)\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb6DoGaP31md",
        "outputId": "92b13b78-5769-430d-e484-57e59c0f6b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Red creada\n",
            "arquitectura:\n",
            "CNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(9, 9), stride=(1, 1), padding=same)\n",
            "    (1): Tanh()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (drop): Dropout2d(p=0.7, inplace=False)\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(16, 16, kernel_size=(9, 9), stride=(1, 1), padding=same)\n",
            "    (1): Tanh()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(16, 16, kernel_size=(9, 9), stride=(1, 1), padding=same)\n",
            "    (1): Tanh()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=144, out_features=10, bias=True)\n",
            ")\n",
            "Número total de capas de CNN (conv+act+polling) + finales :  5\n",
            "Número total de parámetros a entrenar:  44266\n"
          ]
        }
      ],
      "source": [
        "# instancio modelo\n",
        "model = CNN()\n",
        "\n",
        "# armo config (OJO!! NO ESTÁ AUTOMATIZADO!)\n",
        "model_config = {\n",
        "        \"num_layers\": 4,\n",
        "        \"kernel_size\": 9,\n",
        "        \"dropout\": 0.7,\n",
        "        \"n_channels\": [16, 16, 16],\n",
        "        \"architecture\": model.__class__.__name__\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYgRNa9M8cjI",
        "outputId": "421b44dd-c7f6-4d28-b6d5-6137942eeb1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'num_layers': 4,\n",
              " 'kernel_size': 9,\n",
              " 'dropout': 0.7,\n",
              " 'n_channels': [16, 16, 16],\n",
              " 'architecture': 'CNN'}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4tEn-XqHVZ7"
      },
      "source": [
        "## 4. Veamos que las dimensiones sean consistentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q44d3Ftwokla",
        "outputId": "d3456984-d520-4aac-eafe-b3df5981b1a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "CNN                                      [12, 10]                  --\n",
              "├─Sequential: 1-1                        [12, 16, 14, 14]          --\n",
              "│    └─Conv2d: 2-1                       [12, 16, 28, 28]          1,312\n",
              "│    └─Tanh: 2-2                         [12, 16, 28, 28]          --\n",
              "│    └─MaxPool2d: 2-3                    [12, 16, 14, 14]          --\n",
              "├─Dropout2d: 1-2                         [12, 16, 14, 14]          --\n",
              "├─Sequential: 1-3                        [12, 16, 7, 7]            --\n",
              "│    └─Conv2d: 2-4                       [12, 16, 14, 14]          20,752\n",
              "│    └─Tanh: 2-5                         [12, 16, 14, 14]          --\n",
              "│    └─MaxPool2d: 2-6                    [12, 16, 7, 7]            --\n",
              "├─Dropout2d: 1-4                         [12, 16, 7, 7]            --\n",
              "├─Sequential: 1-5                        [12, 16, 3, 3]            --\n",
              "│    └─Conv2d: 2-7                       [12, 16, 7, 7]            20,752\n",
              "│    └─Tanh: 2-8                         [12, 16, 7, 7]            --\n",
              "│    └─MaxPool2d: 2-9                    [12, 16, 3, 3]            --\n",
              "├─Linear: 1-6                            [12, 10]                  1,450\n",
              "==========================================================================================\n",
              "Total params: 44,266\n",
              "Trainable params: 44,266\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 73.37\n",
              "==========================================================================================\n",
              "Input size (MB): 0.04\n",
              "Forward/backward pass size (MB): 1.58\n",
              "Params size (MB): 0.18\n",
              "Estimated Total Size (MB): 1.80\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torchinfo.summary(model, input_size=( 12, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoB3GvDtGUgY"
      },
      "source": [
        "## 5. Armo las funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9w-WYfZd-Yvc"
      },
      "outputs": [],
      "source": [
        "def train_model_wandb(\n",
        "        model,\n",
        "        optimizer,\n",
        "        train_loader,\n",
        "        eval_loader,\n",
        "        loss_module,\n",
        "        config = model_config,\n",
        "        scheduler_type=\"step\",\n",
        "        patience=3,\n",
        "        patience_factor=0.01,\n",
        "        num_epochs=3\n",
        "):\n",
        "    # Initialize Weights & Biases\n",
        "    wandb.init(project=\"test\", config=model_config)\n",
        "\n",
        "    ## Set device\n",
        "    model.to(device)\n",
        "\n",
        "    # Set metric for callbacks\n",
        "    best_eval = 0\n",
        "    pt_epoch = 0\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Epoch Progress\"):\n",
        "        ### Training Phase ###\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_samples = 0\n",
        "\n",
        "        for batch_idx, data in enumerate(train_loader):\n",
        "\n",
        "            X, y = data\n",
        "            # Move input data to device (if using GPU)\n",
        "            data_inputs = X.to(device)\n",
        "            data_labels = y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)  # Ensure shape consistency\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_module(preds, data_labels)\n",
        "\n",
        "            # Zero gradients, backpropagate, and update weights\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track loss for this batch\n",
        "            batch_loss = loss.item()\n",
        "            train_loss += batch_loss\n",
        "\n",
        "            # Compute accuracy (assuming classification task)\n",
        "            if preds.ndim == 2:  # Softmax case\n",
        "                preds_classes = preds.argmax(dim=1)\n",
        "            else:  # Sigmoid case (binary classification)\n",
        "                preds_classes = (preds > 0.5).long()\n",
        "\n",
        "            train_correct += (preds_classes == data_labels).sum().item()\n",
        "            train_samples += data_labels.size(0)\n",
        "\n",
        "            # Log batch-wise metrics\n",
        "            wandb.log({\n",
        "                \"batch_loss\": batch_loss,\n",
        "                \"batch_step\": epoch * len(train_loader) + batch_idx,\n",
        "                \"batch_lr\": optimizer.param_groups[0]['lr'],\n",
        "            })\n",
        "\n",
        "            # Step-level callback\n",
        "            if scheduler_type==\"step\":\n",
        "                scheduler.step()\n",
        "\n",
        "        # Compute training metrics\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy = train_correct / train_samples\n",
        "\n",
        "        ### Evaluation Phase ###\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "        eval_correct = 0\n",
        "        eval_samples = 0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient tracking\n",
        "            for data in eval_loader:\n",
        "\n",
        "                X, y = data\n",
        "                data_inputs = X.to(device)\n",
        "                data_labels = y.to(device)\n",
        "\n",
        "                preds = model(data_inputs)\n",
        "                preds = preds.squeeze(dim=1)\n",
        "\n",
        "                loss = loss_module(preds, data_labels)\n",
        "                eval_loss += loss.item()\n",
        "\n",
        "                if preds.ndim == 2:\n",
        "                    preds_classes = preds.argmax(dim=1)\n",
        "                else:\n",
        "                    preds_classes = (preds > 0.5).long()\n",
        "\n",
        "                eval_correct += (preds_classes == data_labels).sum().item()\n",
        "                eval_samples += data_labels.size(0)\n",
        "\n",
        "        # Compute evaluation metrics\n",
        "        eval_loss /= len(eval_loader)\n",
        "        eval_accuracy = eval_correct / eval_samples\n",
        "\n",
        "        # Log epoch-level metrics\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_accuracy,\n",
        "            \"eval_loss\": eval_loss,\n",
        "            \"eval_accuracy\": eval_accuracy,\n",
        "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        })\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Train Acc: {train_accuracy:.4f} | Eval Loss: {eval_loss:.4f} - Eval Acc: {eval_accuracy:.4f}\")\n",
        "\n",
        "        # Callbacks\n",
        "        ## Learning rate scheduler\n",
        "        if scheduler_type==\"epoch\":\n",
        "            scheduler.step()\n",
        "        ## Early stopping\n",
        "        if eval_accuracy>=best_eval*(1+patience_factor):\n",
        "            best_eval = eval_accuracy\n",
        "            pt_epoch = 0\n",
        "        else:\n",
        "            pt_epoch += 1\n",
        "            if pt_epoch>=patience:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs} - Training interrupted due to early stopping condition.\")\n",
        "                wandb.finish()\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs} - Current epochs without validation metric improvement {pt_epoch}. {patience-pt_epoch} remaining before stopping.\")\n",
        "\n",
        "    # Finish W&B run\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "hpphYbIC_LQa"
      },
      "outputs": [],
      "source": [
        "# definimo optimizer y la función de pérdida\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "# paso al config el optimizador\n",
        "model_config[\"optimizer\"] = optimizer.__class__.__name__\n",
        "\n",
        "# defino función de perdida\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Scheduler\n",
        "scheduler_type = \"step\"\n",
        "steps_per_epoch = len(dataloader['train'])\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=steps_per_epoch, T_mult=2, eta_min=1e-5)\n",
        "#scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-5, max_lr=1e-2, step_size_up=steps_per_epoch, mode=\"triangular2\")\n",
        "\n",
        "## Si queremos aplicar cambios al learning rate al final de cada epoch, usamos como ejemplo otro como este:\n",
        "#scheduler_type = \"epoch\"\n",
        "#scheduler = ExponentialLR(optimizer, gamma=0.99) # https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cW5H0nX99DW"
      },
      "source": [
        "##6. Entrenamiento WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "id": "47ad8Ydj-ePT",
        "outputId": "3654edeb-e1db-45db-8bef-eaa1b5a450e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\UBA\\aprendizaje_profundo\\clase_5\\jupyter_notebooks\\wandb\\run-20250408_150244-4xzs1mf6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mmaillot/test/runs/4xzs1mf6' target=\"_blank\">celestial-snowball-8</a></strong> to <a href='https://wandb.ai/mmaillot/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mmaillot/test' target=\"_blank\">https://wandb.ai/mmaillot/test</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mmaillot/test/runs/4xzs1mf6' target=\"_blank\">https://wandb.ai/mmaillot/test/runs/4xzs1mf6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:   7%|▋         | 1/15 [00:18<04:13, 18.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 - Train Loss: 1.4575 - Train Acc: 0.5870 | Eval Loss: 0.8227 - Eval Acc: 0.7773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  13%|█▎        | 2/15 [00:36<03:56, 18.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/15 - Train Loss: 0.7551 - Train Acc: 0.7918 | Eval Loss: 0.4327 - Eval Acc: 0.8733\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  20%|██        | 3/15 [00:54<03:39, 18.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/15 - Train Loss: 0.5608 - Train Acc: 0.8459 | Eval Loss: 0.3773 - Eval Acc: 0.8885\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  27%|██▋       | 4/15 [01:12<03:19, 18.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/15 - Train Loss: 0.4639 - Train Acc: 0.8715 | Eval Loss: 0.2645 - Eval Acc: 0.9206\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  33%|███▎      | 5/15 [01:30<03:01, 18.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/15 - Train Loss: 0.3774 - Train Acc: 0.8936 | Eval Loss: 0.2218 - Eval Acc: 0.9323\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  40%|████      | 6/15 [01:49<02:44, 18.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/15 - Train Loss: 0.3375 - Train Acc: 0.9043 | Eval Loss: 0.2048 - Eval Acc: 0.9386\n",
            "Epoch 6/15 - Current epochs without validation metric improvement 1. 2 remaining before stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  47%|████▋     | 7/15 [02:07<02:25, 18.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/15 - Train Loss: 0.3203 - Train Acc: 0.9090 | Eval Loss: 0.1994 - Eval Acc: 0.9396\n",
            "Epoch 7/15 - Current epochs without validation metric improvement 2. 1 remaining before stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  53%|█████▎    | 8/15 [02:24<02:06, 18.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/15 - Train Loss: 0.3044 - Train Acc: 0.9121 | Eval Loss: 0.1747 - Eval Acc: 0.9452\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  60%|██████    | 9/15 [02:42<01:48, 18.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/15 - Train Loss: 0.2746 - Train Acc: 0.9199 | Eval Loss: 0.1556 - Eval Acc: 0.9514\n",
            "Epoch 9/15 - Current epochs without validation metric improvement 1. 2 remaining before stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  67%|██████▋   | 10/15 [03:00<01:29, 17.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/15 - Train Loss: 0.2548 - Train Acc: 0.9258 | Eval Loss: 0.1451 - Eval Acc: 0.9545\n",
            "Epoch 10/15 - Current epochs without validation metric improvement 2. 1 remaining before stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  73%|███████▎  | 11/15 [03:18<01:11, 17.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/15 - Train Loss: 0.2416 - Train Acc: 0.9283 | Eval Loss: 0.1378 - Eval Acc: 0.9556\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  80%|████████  | 12/15 [03:36<00:53, 17.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/15 - Train Loss: 0.2274 - Train Acc: 0.9333 | Eval Loss: 0.1323 - Eval Acc: 0.9573\n",
            "Epoch 12/15 - Current epochs without validation metric improvement 1. 2 remaining before stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  87%|████████▋ | 13/15 [03:54<00:35, 17.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/15 - Train Loss: 0.2244 - Train Acc: 0.9334 | Eval Loss: 0.1294 - Eval Acc: 0.9581\n",
            "Epoch 13/15 - Current epochs without validation metric improvement 2. 1 remaining before stopping.\n",
            "Epoch 14/15 - Train Loss: 0.2194 - Train Acc: 0.9356 | Eval Loss: 0.1282 - Eval Acc: 0.9582\n",
            "Epoch 14/15 - Training interrupted due to early stopping condition.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>█▄▄▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▁▁▂▂▂▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▂</td></tr><tr><td>batch_lr</td><td>▅▅▄▃▁▅▅▄▄▃██▇▇▆▆▆▅▅▅▄▃▂▂▁██▇▇▆▅▅▄▃▃▂▂▂▂▁</td></tr><tr><td>batch_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>eval_accuracy</td><td>▁▅▅▇▇▇▇▇██████</td></tr><tr><td>eval_loss</td><td>█▄▄▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>█▄█▇▄▂██▇▆▄▃▂▁</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.12082</td></tr><tr><td>batch_lr</td><td>1e-05</td></tr><tr><td>batch_step</td><td>13131</td></tr><tr><td>epoch</td><td>13</td></tr><tr><td>eval_accuracy</td><td>0.9582</td></tr><tr><td>eval_loss</td><td>0.1282</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>train_accuracy</td><td>0.93557</td></tr><tr><td>train_loss</td><td>0.21941</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">celestial-snowball-8</strong> at: <a href='https://wandb.ai/mmaillot/test/runs/4xzs1mf6' target=\"_blank\">https://wandb.ai/mmaillot/test/runs/4xzs1mf6</a><br> View project at: <a href='https://wandb.ai/mmaillot/test' target=\"_blank\">https://wandb.ai/mmaillot/test</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250408_150244-4xzs1mf6\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Progress:  87%|████████▋ | 13/15 [04:13<00:39, 19.52s/it]\n"
          ]
        }
      ],
      "source": [
        "# corremos la función de entrenamiento con todos los argumentos correspondientes\n",
        "train_model_wandb(model, optimizer, dataloader['train'], dataloader['test'], loss_fn, scheduler_type=scheduler_type, num_epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHcr6aDtHNfc"
      },
      "source": [
        "## 7. Vemos que funcione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "k_jFvJ603PC-",
        "outputId": "2961c411-8eeb-4eb8-f902-cc083df20d3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "tensor(-0.4242)\n",
            "tensor(2.8215)\n",
            "tensor(-0.4242)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño imagen de entrada a red:  torch.Size([1, 1, 28, 28])\n",
            "Predición del modelo:\n",
            "tensor([[-2.2373,  5.4469, -0.6796,  0.2946, -0.8774, -0.0928, -0.0867, -0.0913,\n",
            "          0.2713, -0.3452]], device='cuda:0')\n",
            "\n",
            "softmax de predicción:\n",
            "tensor([[4.4632e-04, 9.7018e-01, 2.1191e-03, 5.6137e-03, 1.7388e-03, 3.8108e-03,\n",
            "         3.8338e-03, 3.8165e-03, 5.4846e-03, 2.9607e-03]], device='cuda:0')\n",
            "\n",
            "El numero es un:  1\n"
          ]
        }
      ],
      "source": [
        "# corremos 1 dato, a ver como lo clasifica...\n",
        "# generamos un batch del dataloader\n",
        "test_features, test_labels = next(iter(dataloader['test']))\n",
        "\n",
        "# item a usar k\n",
        "k = 14\n",
        "\n",
        "# verifico las dimensiones y los valores que toma algun pixel.\n",
        "samp_img = test_features[k]\n",
        "print(samp_img.shape)\n",
        "print(samp_img[0][0][0])\n",
        "print(torch.max(samp_img))\n",
        "print(torch.min(samp_img))\n",
        "\n",
        "# ploteo la imagen\n",
        "plt.imshow(samp_img.squeeze(), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# preparo para pasarla a la red (model) asi predice.\n",
        "samp_imp = samp_img.unsqueeze(0) # agrego la batch dim\n",
        "samp_img = samp_img.unsqueeze(0).to(device)\n",
        "print('Tamaño imagen de entrada a red: ', samp_img.shape)\n",
        "\n",
        "# la paso al modelo\n",
        "model.to(device)\n",
        "model.eval()\n",
        "y_hat = model(samp_img)\n",
        "print('Predición del modelo:')\n",
        "print(y_hat.detach())\n",
        "print()\n",
        "print('softmax de predicción:')\n",
        "print(torch.nn.functional.softmax(y_hat, dim=1).detach())\n",
        "print()\n",
        "print(f'El numero es un: ', torch.argmax(y_hat, axis=1).item())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
